# Web Scraping from Wikipedia using Python â€“ A Complete Guide

## Introduction

This project demonstrates web scraping techniques using Python to extract structured data from Wikipedia. Web scraping automates the process of data extraction from web pages, allowing for efficient gathering and storage of information locally in formats like Excel sheets or JSON.


## Installation Instructions

To set up the necessary libraries for this project, follow these steps:

1. **Install `virtualenv`** (if not already installed):

   ```bash
   pip install virtualenv
   ```

2. **Create a virtual environment** (optional but recommended):

   ```bash
   virtualenv venv
   ```
   
   Activate the virtual environment:

   - **On Windows:**

     ```bash
     venv\Scripts\activate
     ```

   - **On macOS and Linux:**

     ```bash
     source venv/bin/activate
     ```

3. **Install the required libraries**:

   ```bash
   python -m pip install selenium
   python -m pip install requests
   python -m pip install urllib3
   ```

## Libraries

### 1. Selenium

- **Description**: Web automation and testing framework.
- **Installation**: `python -m pip install selenium`

### 2. Requests

- **Description**: HTTP library for making requests to web servers.
- **Installation**: `python -m pip install requests`

### 3. urllib3

- **Description**: HTTP library with thread-safe connection pooling, file posting, and more.
- **Installation**: `python -m pip install urllib3`

---

## Tools and Setup

To run this project, ensure Python is installed on your system along with the required libraries:

```bash
pip install requests lxml beautifulsoup4 selenium scrapy
```

## Running the Scraping Process

1. **Choose a Library**: Select the appropriate library based on your scraping needs.
   
2. **Scraping Logic**: Write code to fetch and parse data from Wikipedia pages.

3. **Data Extraction**: Save the extracted data locally in formats like Excel sheets or JSON.


## Conclusion

This project provides a comprehensive guide to web scraping with Python using various libraries like Requests, lxml, Beautiful Soup, Selenium, and Scrapy. By leveraging these tools, you can automate data extraction from web pages efficiently.

## Acknowledgments

- **Requests**: https://docs.python-requests.org/
- **lxml**: https://lxml.de/
- **Beautiful Soup**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **Selenium**: https://www.selenium.dev/documentation/en/
- **Scrapy**: https://docs.scrapy.org/
- **GeekforGeek**: https://www.geeksforgeeks.org/web-scraping-from-wikipedia-using-python-a-complete-guide/

---
